{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BthRezPlR9Ow"
   },
   "source": [
    "# Text Classification Model for Disaster Tweets Classification Using TensorFlow Take 1\n",
    "### David Lowe\n",
    "### February 12, 2021\n",
    "\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: This project aims to construct a text classification model using a neural network and document the end-to-end steps using a template. The Disaster Tweets Classification dataset is a binary classification situation where we attempt to predict one of the two possible outcomes.\n",
    "\n",
    "INTRODUCTION: Twitter has become an important communication channel in times of emergency. The ubiquitous nature of smartphones enables people to announce an emergency they are observing in real-time. Because of this, more agencies are interested in programmatically monitoring Twitter. In this practice Kaggle competition, we want to build a machine learning model that predicts which Tweets are about real disasters and which ones are not. This dataset was created by Figure-Eight and shared initially on their 'Data for Everyone' website.\n",
    "\n",
    "In this Take1 iteration, we will deploy a bag-of-words model to classify the Tweets. We will also make predictions on Kaggle's test dataset and submit the results for evaluation.\n",
    "\n",
    "ANALYSIS: In this Take1 iteration, the bag-of-words model's performance achieved an average accuracy score of 75.49% after 20 epochs with five iterations of cross-validation. Furthermore, the final model processed the test dataset with an accuracy measurement of 75.02%.\n",
    "\n",
    "CONCLUSION: In this modeling iteration, the bag-of-words TensorFlow model appeared to be suitable for modeling this dataset. We should consider experimenting with TensorFlow for further modeling.\n",
    "\n",
    "Dataset Used: Sentiment Labelled Sentences\n",
    "\n",
    "Dataset ML Model: Binary class text classification with text-oriented features\n",
    "\n",
    "Dataset Reference: https://www.kaggle.com/c/nlp-getting-started/\n",
    "\n",
    "A deep-learning text classification project generally can be broken down into five major tasks:\n",
    "\n",
    "1. Prepare Environment\n",
    "2. Load and Prepare Text Data\n",
    "3. Define and Train Models\n",
    "4. Evaluate and Optimize Models\n",
    "5. Finalize Model and Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZamo1ynR9Oz"
   },
   "source": [
    "# Task 1 - Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2N3QHMbUR9Oz",
    "outputId": "2e8152bf-215e-4bc1-f19f-3a208e1c9bfa"
   },
   "outputs": [],
   "source": [
    "# # Install the packages to support accessing environment variable and SQL databases\n",
    "# !pip install python-dotenv PyMySQL boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOWG9MDAR9O2",
    "outputId": "1e1308b7-ac4e-4b11-deeb-4e3877712fa7"
   },
   "outputs": [],
   "source": [
    "# # Retrieve GPU configuration information from Colab\n",
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#     print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "#     print('and then re-execute this cell.')\n",
    "# else:\n",
    "#     print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ryCwWMZuR9O4",
    "outputId": "6bb39b0f-00e8-440a-e98c-1947cfd617b4"
   },
   "outputs": [],
   "source": [
    "# # Retrieve memory configuration information from Colab\n",
    "# from psutil import virtual_memory\n",
    "# ram_gb = virtual_memory().total / 1e9\n",
    "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "# if ram_gb < 20:\n",
    "#     print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
    "#     print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "#     print('re-execute this cell.')\n",
    "# else:\n",
    "#     print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WfPiU9LaR9O7",
    "outputId": "6a68c8f3-d61e-4bb9-e6af-e2f946c184f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of available CPUs is: 4\n"
     ]
    }
   ],
   "source": [
    "# Retrieve CPU information from the system\n",
    "ncpu = !nproc\n",
    "print(\"The number of available CPUs is:\", ncpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUw0fDwqR9O-"
   },
   "source": [
    "## 1.a) Load libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UpJD2SzRR9O-"
   },
   "outputs": [],
   "source": [
    "# Set the random seed number for reproducible results\n",
    "seedNum = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yDoBs19R9PA",
    "outputId": "5c5761c9-d774-4ba8-ba19-79a10c9f6a6e"
   },
   "outputs": [],
   "source": [
    "# Load libraries and packages\n",
    "import random\n",
    "random.seed(seedNum)\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import shutil\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcIz9xXxR9PC"
   },
   "source": [
    "## 1.b) Set up the controlling parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YecJPSrfR9PC",
    "outputId": "bc0304a5-e471-4568-826e-750a5220e2a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "TensorFlow version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the number of CPU cores available for multi-thread processing\n",
    "n_jobs = 1\n",
    "\n",
    "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set the verbose level for program execution output\n",
    "verbose = False\n",
    "\n",
    "# Set Pandas options\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# Set the percentage sizes for splitting the dataset\n",
    "TEST_SET_SIZE = 0.2\n",
    "VAL_SET_SIZE = 0.25\n",
    "\n",
    "# Set the number of folds for cross validation\n",
    "N_FOLDS = 5\n",
    "N_ITERATIONS = 1\n",
    "\n",
    "# Set various default modeling parameters\n",
    "DEFAULT_LOSS = 'binary_crossentropy'\n",
    "DEFAULT_METRICS = ['accuracy']\n",
    "DEFAULT_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "DEFAULT_INITIALIZER = tf.keras.initializers.GlorotUniform(seed=seedNum)\n",
    "EPOCH_NUMBER = 20\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Define the labels to use for graphing the data\n",
    "train_metric = \"accuracy\"\n",
    "validation_metric = \"val_accuracy\"\n",
    "train_loss = \"loss\"\n",
    "validation_loss = \"val_loss\"\n",
    "\n",
    "# Check the number of GPUs accessible through TensorFlow\n",
    "print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Print out the TensorFlow version for confirmation\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ggPdMUQfR9PE"
   },
   "outputs": [],
   "source": [
    "# Set up the parent directory location for loading the dotenv files\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# gdrivePrefix = '/content/gdrive/My Drive/Colab_Downloads/'\n",
    "# env_path = '/content/gdrive/My Drive/Colab Notebooks/'\n",
    "# dotenv_path = env_path + \"python_script.env\"\n",
    "# load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Set up the dotenv file for retrieving environment variables\n",
    "# env_path = \"/Users/david/PycharmProjects/\"\n",
    "# dotenv_path = env_path + \"python_script.env\"\n",
    "# load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1j-y-SvmR9PG"
   },
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def status_notify(msg_text):\n",
    "    access_key = os.environ.get('SNS_ACCESS_KEY')\n",
    "    secret_key = os.environ.get('SNS_SECRET_KEY')\n",
    "    aws_region = os.environ.get('SNS_AWS_REGION')\n",
    "    topic_arn = os.environ.get('SNS_TOPIC_ARN')\n",
    "    if (access_key is None) or (secret_key is None) or (aws_region is None):\n",
    "        sys.exit(\"Incomplete notification setup info. Script Processing Aborted!!!\")\n",
    "    sns = boto3.client('sns', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=aws_region)\n",
    "    response = sns.publish(TopicArn=topic_arn, Message=msg_text)\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] != 200 :\n",
    "        print('Status notification not OK with HTTP status code:', response['ResponseMetadata']['HTTPStatusCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KDY6p9pWR9PI"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 1 - Prepare Environment has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YQtFgiRNR9PJ"
   },
   "outputs": [],
   "source": [
    "# Reset the random number generators\n",
    "def reset_random(x):\n",
    "    random.seed(x)\n",
    "    np.random.seed(x)\n",
    "    tf.random.set_seed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "id": "7auUp3NTR9PL",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 1 - Prepare Environment completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkkVYtk_R9PN"
   },
   "source": [
    "# Task 2 - Load and Prepare Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "id": "pX4nz8fYR9PN",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 2 - Load and Prepare Text Data has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a) Download Text Data Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text  target\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1\n",
      "5   8     NaN      NaN  #RockyFire Update => California Hwy. 20 closed...       1\n",
      "6  10     NaN      NaN  #flood #disaster Heavy rain causes flash flood...       1\n",
      "7  13     NaN      NaN  I'm on top of the hill and I can see a fire in...       1\n",
      "8  14     NaN      NaN  There's an emergency evacuation happening now ...       1\n",
      "9  15     NaN      NaN  I'm afraid that the tornado is coming to our a...       1\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'https://dainesanalytics.com/datasets/kaggle-nlp-disaster-tweets/train.csv'\n",
    "Xy_train = pd.read_csv(dataset_path)\n",
    "\n",
    "# Take a peek at the dataframe after import\n",
    "print(Xy_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  target\n",
      "0  Our Deeds are the Reason of this #earthquake M...       1\n",
      "1             Forest fire near La Ronge Sask. Canada       1\n",
      "2  All residents asked to 'shelter in place' are ...       1\n",
      "3  13,000 people receive #wildfires evacuation or...       1\n",
      "4  Just got sent this photo from Ruby #Alaska as ...       1\n",
      "5  #RockyFire Update => California Hwy. 20 closed...       1\n",
      "6  #flood #disaster Heavy rain causes flash flood...       1\n",
      "7  I'm on top of the hill and I can see a fire in...       1\n",
      "8  There's an emergency evacuation happening now ...       1\n",
      "9  I'm afraid that the tornado is coming to our a...       1\n"
     ]
    }
   ],
   "source": [
    "# Dropping redundant features\n",
    "Xy_train.drop(columns=['id','keyword','location'], inplace=True)\n",
    "\n",
    "# Take a peek at the dataframe after cleaning\n",
    "print(Xy_train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b) Splitting Data for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xy_train_df.shape: (7613, 2) X_train_df.shape: (7613,) y_train_df.shape: (7613,)\n"
     ]
    }
   ],
   "source": [
    "X_train_df = Xy_train.iloc[:,0]\n",
    "y_train_df = Xy_train.iloc[:,1]\n",
    "print(\"Xy_train_df.shape: {} X_train_df.shape: {} y_train_df.shape: {}\".format(Xy_train.shape, X_train_df.shape, y_train_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.c) Load Document and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences and add to vocab\n",
    "def add_sentence_to_vocab(sentence, vocab):\n",
    "    # clean doc\n",
    "    tokens = clean_sentence(sentence)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def build_vocabulary(comments, vocab):\n",
    "    # walk through all comments in the dataframe\n",
    "    for i in range(len(comments)):\n",
    "        # add comments to vocab\n",
    "        sentence = comments.iloc[i]\n",
    "        add_sentence_to_vocab(sentence, vocab)\n",
    "        if verbose : print('Processing comment:', sentence)\n",
    "        if ((i+1) % 100) == 0 : print(i+1, 'comments processed so far.')\n",
    "    print('Total number of comments loaded into the vocabulary:', i+1, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 comments processed so far.\n",
      "200 comments processed so far.\n",
      "300 comments processed so far.\n",
      "400 comments processed so far.\n",
      "500 comments processed so far.\n",
      "600 comments processed so far.\n",
      "700 comments processed so far.\n",
      "800 comments processed so far.\n",
      "900 comments processed so far.\n",
      "1000 comments processed so far.\n",
      "1100 comments processed so far.\n",
      "1200 comments processed so far.\n",
      "1300 comments processed so far.\n",
      "1400 comments processed so far.\n",
      "1500 comments processed so far.\n",
      "1600 comments processed so far.\n",
      "1700 comments processed so far.\n",
      "1800 comments processed so far.\n",
      "1900 comments processed so far.\n",
      "2000 comments processed so far.\n",
      "2100 comments processed so far.\n",
      "2200 comments processed so far.\n",
      "2300 comments processed so far.\n",
      "2400 comments processed so far.\n",
      "2500 comments processed so far.\n",
      "2600 comments processed so far.\n",
      "2700 comments processed so far.\n",
      "2800 comments processed so far.\n",
      "2900 comments processed so far.\n",
      "3000 comments processed so far.\n",
      "3100 comments processed so far.\n",
      "3200 comments processed so far.\n",
      "3300 comments processed so far.\n",
      "3400 comments processed so far.\n",
      "3500 comments processed so far.\n",
      "3600 comments processed so far.\n",
      "3700 comments processed so far.\n",
      "3800 comments processed so far.\n",
      "3900 comments processed so far.\n",
      "4000 comments processed so far.\n",
      "4100 comments processed so far.\n",
      "4200 comments processed so far.\n",
      "4300 comments processed so far.\n",
      "4400 comments processed so far.\n",
      "4500 comments processed so far.\n",
      "4600 comments processed so far.\n",
      "4700 comments processed so far.\n",
      "4800 comments processed so far.\n",
      "4900 comments processed so far.\n",
      "5000 comments processed so far.\n",
      "5100 comments processed so far.\n",
      "5200 comments processed so far.\n",
      "5300 comments processed so far.\n",
      "5400 comments processed so far.\n",
      "5500 comments processed so far.\n",
      "5600 comments processed so far.\n",
      "5700 comments processed so far.\n",
      "5800 comments processed so far.\n",
      "5900 comments processed so far.\n",
      "6000 comments processed so far.\n",
      "6100 comments processed so far.\n",
      "6200 comments processed so far.\n",
      "6300 comments processed so far.\n",
      "6400 comments processed so far.\n",
      "6500 comments processed so far.\n",
      "6600 comments processed so far.\n",
      "6700 comments processed so far.\n",
      "6800 comments processed so far.\n",
      "6900 comments processed so far.\n",
      "7000 comments processed so far.\n",
      "7100 comments processed so far.\n",
      "7200 comments processed so far.\n",
      "7300 comments processed so far.\n",
      "7400 comments processed so far.\n",
      "7500 comments processed so far.\n",
      "7600 comments processed so far.\n",
      "Total number of comments loaded into the vocabulary: 7613 \n",
      "\n",
      "The total number of words in the vocabulary: 20932\n",
      "The top 50 words in the vocabulary:\n",
      " [('The', 581), ('like', 321), ('amp', 298), ('Im', 238), ('via', 213), ('get', 184), ('fire', 174), ('people', 164), ('In', 159), ('one', 154), ('dont', 145), ('would', 119), ('California', 113), ('This', 110), ('News', 110), ('To', 109), ('RT', 105), ('new', 105), ('got', 104), ('You', 104), ('know', 104), ('New', 103), ('video', 103), ('disaster', 101), ('My', 97), ('back', 97), ('buildings', 96), ('time', 95), ('If', 95), ('going', 94), ('US', 93), ('YouTube', 92), ('burning', 92), ('killed', 90), ('day', 89), ('still', 89), ('see', 88), ('fires', 88), ('THE', 86), ('go', 86), ('bomb', 85), ('Hiroshima', 85), ('Is', 83), ('crash', 82), ('cant', 82), ('think', 81), ('attack', 80), ('Its', 79), ('suicide', 79), ('body', 78)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "build_vocabulary(X_train_df, vocab)\n",
    "# print the size of the vocab\n",
    "print('The total number of words in the vocabulary:', len(vocab))\n",
    "# print the top words in the vocab\n",
    "top_words = 50\n",
    "print('The top', top_words, 'words in the vocabulary:\\n', vocab.most_common(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words with the minimum appearance: 20932\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence\n",
    "min_occurane = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print('The number of words with the minimum appearance:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocabulary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocabulary: 20932\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocabulary.txt'\n",
    "vocab_text = load_doc(vocab_filename)\n",
    "vocab_words = vocab_text.split()\n",
    "vocab = set(vocab_words)\n",
    "print('Number of tokens in the vocabulary:', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.d) Create Tokenizer and Encode the Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens\n",
    "def comment_to_line(sentence, vocab):\n",
    "    # clean sentence\n",
    "    tokens = clean_sentence(sentence)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all docs in a directory\n",
    "def process_comments_to_lines(comments, vocab):\n",
    "    lines = list()\n",
    "    # walk through all comments in the dataframe\n",
    "    for i in range(len(comments)):\n",
    "        # load and clean the comments\n",
    "        sentence = comments.iloc[i]\n",
    "        line = comment_to_line(sentence, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments processed: 7613\n"
     ]
    }
   ],
   "source": [
    "# Load all training cases\n",
    "training_docs = process_comments_to_lines(X_train_df, vocab)\n",
    "print('Number of comments processed:', len(training_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare bag of words encoding of docs\n",
    "def encode_training_data(train_docs, mode='binary'):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    train_encoded = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    return train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare bag of words encoding of docs\n",
    "def encode_test_data(train_docs, test_docs, mode='binary'):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode test data set\n",
    "    test_encoded = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (7613, 16977)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train = encode_training_data(training_docs)\n",
    "print('The shape of the encoded training dataset:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training classes: (7613,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train_df.values.ravel()\n",
    "print('The shape of the encoded training classes:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JdeBWbYeR9Pi"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 2 - Load and Prepare Text Data completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGVOq6P6R9Pj"
   },
   "source": [
    "# Task 3 - Define and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "W52KEpCWR9Pk"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 3 - Define and Train Models has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the default numbers of input/output for modeling\n",
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the baseline model for benchmarking\n",
    "def create_nn_model(n_inputs=num_inputs, n_outputs=num_outputs, layer1_nodes=64, layer2_nodes=32, \n",
    "                    opt_param=DEFAULT_OPTIMIZER, init_param=DEFAULT_INITIALIZER, loss_param=DEFAULT_LOSS, metrics_param=DEFAULT_METRICS):\n",
    "    nn_model = keras.Sequential([\n",
    "        keras.layers.Dense(layer1_nodes, input_shape=(n_inputs,), activation='relu', kernel_initializer=init_param),\n",
    "        keras.layers.Dense(layer2_nodes, activation='relu', kernel_initializer=init_param),\n",
    "        keras.layers.Dense(n_outputs, activation='sigmoid', kernel_initializer=init_param)\n",
    "    ])\n",
    "    nn_model.compile(loss=loss_param, optimizer=opt_param, metrics=metrics_param)\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 76.10%\n",
      "Accuracy measurement from iteration 2 >>> 75.18%\n",
      "Accuracy measurement from iteration 3 >>> 75.18%\n",
      "Accuracy measurement from iteration 4 >>> 75.62%\n",
      "Accuracy measurement from iteration 5 >>> 75.36%\n",
      "Average model accuracy from all iterations: 75.49% (0.35%)\n",
      "Total time for model fitting and cross validating: 0:32:43.298675\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=N_FOLDS, n_repeats=N_ITERATIONS, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    baseline_model = create_nn_model()\n",
    "    baseline_model.fit(feature_train, target_train, epochs=EPOCH_NUMBER, batch_size=BATCH_SIZE, verbose=0)\n",
    "    model_metric = baseline_model.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "id": "5vzQkIfyR9Ps",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 3 - Define and Train Models completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmuo6X06R9Pu"
   },
   "source": [
    "# Task 4 - Evaluate and Optimize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "id": "h-Xp746uR9Pu",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 4 - Evaluate and Optimize Models has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) Alternate Model One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (7613, 16977)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train = encode_training_data(training_docs, 'count')\n",
    "print('The shape of the encoded training dataset:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 75.57%\n",
      "Accuracy measurement from iteration 2 >>> 75.51%\n",
      "Accuracy measurement from iteration 3 >>> 74.85%\n",
      "Accuracy measurement from iteration 4 >>> 75.95%\n",
      "Accuracy measurement from iteration 5 >>> 75.23%\n",
      "Average model accuracy from all iterations: 75.42% (0.37%)\n",
      "Total time for model fitting and cross validating: 0:31:07.032219\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=N_FOLDS, n_repeats=N_ITERATIONS, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    alternate_model_1 = create_nn_model()\n",
    "    alternate_model_1.fit(feature_train, target_train, epochs=EPOCH_NUMBER, batch_size=BATCH_SIZE, verbose=0)\n",
    "    model_metric = alternate_model_1.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) Alternate Model Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (7613, 16977)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train = encode_training_data(training_docs, 'tfidf')\n",
    "print('The shape of the encoded training dataset:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 75.84%\n",
      "Accuracy measurement from iteration 2 >>> 74.52%\n",
      "Accuracy measurement from iteration 3 >>> 75.71%\n",
      "Accuracy measurement from iteration 4 >>> 75.82%\n",
      "Accuracy measurement from iteration 5 >>> 75.49%\n",
      "Average model accuracy from all iterations: 75.48% (0.49%)\n",
      "Total time for model fitting and cross validating: 0:31:01.316173\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=N_FOLDS, n_repeats=N_ITERATIONS, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    alternate_model_2 = create_nn_model()\n",
    "    alternate_model_2.fit(feature_train, target_train, epochs=EPOCH_NUMBER, batch_size=BATCH_SIZE, verbose=0)\n",
    "    model_metric = alternate_model_2.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a) Alternate Model Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training dataset: (7613, 16977)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_train = encode_training_data(training_docs, 'freq')\n",
    "print('The shape of the encoded training dataset:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy measurement from iteration 1 >>> 74.66%\n",
      "Accuracy measurement from iteration 2 >>> 73.67%\n",
      "Accuracy measurement from iteration 3 >>> 74.72%\n",
      "Accuracy measurement from iteration 4 >>> 75.23%\n",
      "Accuracy measurement from iteration 5 >>> 73.06%\n",
      "Average model accuracy from all iterations: 74.27% (0.79%)\n",
      "Total time for model fitting and cross validating: 0:31:00.808002\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "results = list()\n",
    "iteration = 0\n",
    "cv = RepeatedKFold(n_splits=N_FOLDS, n_repeats=N_ITERATIONS, random_state=seedNum)\n",
    "for train_ix, val_ix in cv.split(X_train):\n",
    "    feature_train, feature_validation = X_train[train_ix], X_train[val_ix]\n",
    "    target_train, target_validation = y_train[train_ix], y_train[val_ix]\n",
    "    reset_random(seedNum)\n",
    "    alternate_model_3 = create_nn_model()\n",
    "    alternate_model_3.fit(feature_train, target_train, epochs=EPOCH_NUMBER, batch_size=BATCH_SIZE, verbose=0)\n",
    "    model_metric = alternate_model_3.evaluate(feature_validation, target_validation, verbose=0)[1]\n",
    "    iteration = iteration + 1\n",
    "    print('Accuracy measurement from iteration %d >>> %.2f%%' % (iteration, model_metric*100))\n",
    "    results.append(model_metric)\n",
    "validation_score = np.mean(results)\n",
    "validation_variance = np.std(results)\n",
    "print('Average model accuracy from all iterations: %.2f%% (%.2f%%)' % (validation_score*100, validation_variance*100))\n",
    "print('Total time for model fitting and cross validating:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "6n7j3PwoR9P0"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 4 - Evaluate and Optimize Models completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRcXHaGhR9P1"
   },
   "source": [
    "# Task 5 - Finalize Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "p3tj_9tBR9P2"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 5 - Finalize Model and Make Predictions has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.a) Set up and Train the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.6190 - accuracy: 0.6586\n",
      "Epoch 2/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.4291 - accuracy: 0.8366\n",
      "Epoch 3/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.3294 - accuracy: 0.8735\n",
      "Epoch 4/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.2770 - accuracy: 0.8926\n",
      "Epoch 5/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.2381 - accuracy: 0.9128\n",
      "Epoch 6/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.2064 - accuracy: 0.9250\n",
      "Epoch 7/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.1802 - accuracy: 0.9373\n",
      "Epoch 8/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.1606 - accuracy: 0.9442\n",
      "Epoch 9/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.1421 - accuracy: 0.9509\n",
      "Epoch 10/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.1266 - accuracy: 0.9551\n",
      "Epoch 11/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.1137 - accuracy: 0.9619\n",
      "Epoch 12/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.1025 - accuracy: 0.9645\n",
      "Epoch 13/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0928 - accuracy: 0.9685\n",
      "Epoch 14/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0834 - accuracy: 0.9715\n",
      "Epoch 15/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0785 - accuracy: 0.9737\n",
      "Epoch 16/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0714 - accuracy: 0.9756\n",
      "Epoch 17/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0655 - accuracy: 0.9774\n",
      "Epoch 18/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0600 - accuracy: 0.9789\n",
      "Epoch 19/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0567 - accuracy: 0.9791\n",
      "Epoch 20/20\n",
      "7613/7613 [==============================] - 30s 4ms/step - loss: 0.0534 - accuracy: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa79641b4f0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final model for evaluating the test dataset\n",
    "reset_random(seedNum)\n",
    "final_model = create_nn_model()\n",
    "final_model.fit(X_train, y_train, epochs=EPOCH_NUMBER, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 64)                1086592   \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,088,705\n",
      "Trainable params: 1,088,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the final model\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.b) Create Submission Files for Kaggle Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
      "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
      "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
      "7  22     NaN      NaN                                  Hey! How are you?\n",
      "8  27     NaN      NaN                                   What a nice hat?\n",
      "9  29     NaN      NaN                                          Fuck off!\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'https://dainesanalytics.com/datasets/kaggle-nlp-disaster-tweets/test.csv'\n",
    "X_kaggle_data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Take a peek at the dataframe after import\n",
    "print(X_kaggle_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id target\n",
      "0   0    NaN\n",
      "1   2    NaN\n",
      "2   3    NaN\n",
      "3   9    NaN\n",
      "4  11    NaN\n"
     ]
    }
   ],
   "source": [
    "# Set up the dataframe to capture predictions for Kaggle submission\n",
    "y_submission_kaggle = pd.DataFrame(columns=['id', 'target'])\n",
    "y_submission_kaggle['id'] = X_kaggle_data['id']\n",
    "# X_kaggle_data.drop(columns=['ID'], inplace=True)\n",
    "print(y_submission_kaggle.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_df.shape: (3263,)\n",
      "0                   Just happened a terrible car crash\n",
      "1    Heard about #earthquake is different cities, s...\n",
      "2    there is a forest fire at spot pond, geese are...\n",
      "3             Apocalypse lighting. #Spokane #wildfires\n",
      "4        Typhoon Soudelor kills 28 in China and Taiwan\n",
      "5                   We're shaking...It's an earthquake\n",
      "6    They'd probably still show more life than Arse...\n",
      "7                                    Hey! How are you?\n",
      "8                                     What a nice hat?\n",
      "9                                            Fuck off!\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Dropping redundant features\n",
    "X_kaggle_data.drop(columns=['id','keyword','location'], inplace=True)\n",
    "\n",
    "# Take a peek at the dataframe after cleaning\n",
    "X_test_df = X_kaggle_data.iloc[:,0]\n",
    "print(\"X_test_df.shape: {}\".format(X_test_df.shape))\n",
    "print(X_test_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments processed: 3263\n"
     ]
    }
   ],
   "source": [
    "# load all test/validation cases\n",
    "test_docs = process_comments_to_lines(X_test_df, vocab)\n",
    "print('Number of comments processed:', len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded test dataset: (3263, 16977)\n"
     ]
    }
   ],
   "source": [
    "# encode training and validation datasets\n",
    "X_test = encode_test_data(training_docs, test_docs, 'binary')\n",
    "print('The shape of the encoded test dataset:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263/3263 [==============================] - 4s 1ms/step\n",
      "y_submission_kaggle.shape: (3263, 2)\n",
      "   id  target\n",
      "0   0       1\n",
      "1   2       0\n",
      "2   3       1\n",
      "3   9       1\n",
      "4  11       1\n"
     ]
    }
   ],
   "source": [
    "test_predictions = final_model.predict(X_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "probabilities_kaggle = (test_predictions > 0.5).astype(\"int32\").ravel()\n",
    "y_submission_kaggle['target'] = probabilities_kaggle\n",
    "print(\"y_submission_kaggle.shape: {}\".format(y_submission_kaggle.shape))\n",
    "print(y_submission_kaggle.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed writing output file: submission_20210210-0522.csv\n"
     ]
    }
   ],
   "source": [
    "submission_file = y_submission_kaggle.to_csv(header=True, index=False)\n",
    "filename = 'submission_' + datetime.now().strftime('%Y%m%d-%H%M') + '.csv'\n",
    "with open(filename, 'w') as f:\n",
    "    f.write(submission_file)\n",
    "    print('Completed writing output file: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "S47yeSrOR9P-"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 5 - Finalize Model and Make Predictions completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_kTqSp4R9QA",
    "outputId": "38f72f65-eea4-443f-cc58-62b4d17cb7b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 2:16:26.406983\n"
     ]
    }
   ],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "py_tensorflow_binaryclass_image_classification_example.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
