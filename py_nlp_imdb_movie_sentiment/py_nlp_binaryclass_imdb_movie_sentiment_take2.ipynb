{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BthRezPlR9Ow"
   },
   "source": [
    "# Text Classification Model for IMDB Movie Sentiment Using TensorFlow Take 2\n",
    "### David Lowe\n",
    "### April 2, 2021\n",
    "\n",
    "Template Credit: Adapted from a template made available by Dr. Jason Brownlee of Machine Learning Mastery. [https://machinelearningmastery.com/]\n",
    "\n",
    "SUMMARY: This project aims to construct a text classification model using a neural network and document the end-to-end steps using a template. The IMDB Movie Sentiment dataset is a binary classification situation where we attempt to predict one of the two possible outcomes.\n",
    "\n",
    "INTRODUCTION: This dataset contains 50,000 movie reviews extracted  from IMDB. The researchers have annotated the tweets with labels (0 = negative, 1 = positive) to detect sentiment from the reviews.\n",
    "\n",
    "From iteration Take1, we created a bag-of-words model to perform binary classification (positive or negative) for the Tweets. The Part A script focused on building the model with the training and validation datasets due to memory capacity constraints. Part B focused on testing the model with the training and test datasets.\n",
    "\n",
    "In this Take2 iteration, we will create a word-embedding model to perform binary classification for the Tweets.\n",
    "\n",
    "ANALYSIS: From iteration Take1, the preliminary model's performance achieved an accuracy score of 88.80% on the validation dataset after 10 epochs. Furthermore, the final model processed the test dataset with an accuracy measurement of 89.48%.\n",
    "\n",
    "In this Take2 iteration, the preliminary model's performance achieved an average accuracy score of 88.40% on the validation dataset after 10 epochs. Furthermore, the final model processed the test dataset with an accuracy measurement of 89.66%.\n",
    "\n",
    "CONCLUSION: In this iteration, the word-embedding TensorFlow model appeared to be suitable for modeling this dataset. We should consider experimenting with TensorFlow for further modeling.\n",
    "\n",
    "Dataset Used: IMDB Movie Sentiment\n",
    "\n",
    "Dataset ML Model: Binary class text classification with text-oriented features\n",
    "\n",
    "Dataset Reference: https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format\n",
    "\n",
    "One potential source of performance benchmarks: https://www.kaggle.com/columbine/imdb-dataset-sentiment-analysis-in-csv-format\n",
    "\n",
    "A deep-learning text classification project generally can be broken down into five major tasks:\n",
    "\n",
    "1. Prepare Environment\n",
    "2. Load and Prepare Text Data\n",
    "3. Define and Train Models\n",
    "4. Evaluate and Optimize Models\n",
    "5. Finalize Model and Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZamo1ynR9Oz"
   },
   "source": [
    "# Task 1 - Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2N3QHMbUR9Oz"
   },
   "outputs": [],
   "source": [
    "# # Install the packages to support accessing environment variable and SQL databases\n",
    "# !pip install python-dotenv PyMySQL boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZOWG9MDAR9O2"
   },
   "outputs": [],
   "source": [
    "# # Retrieve GPU configuration information from Colab\n",
    "# gpu_info = !nvidia-smi\n",
    "# gpu_info = '\\n'.join(gpu_info)\n",
    "# if gpu_info.find('failed') >= 0:\n",
    "#     print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "#     print('and then re-execute this cell.')\n",
    "# else:\n",
    "#     print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ryCwWMZuR9O4"
   },
   "outputs": [],
   "source": [
    "# # Retrieve memory configuration information from Colab\n",
    "# from psutil import virtual_memory\n",
    "# ram_gb = virtual_memory().total / 1e9\n",
    "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "# if ram_gb < 20:\n",
    "#     print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
    "#     print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "#     print('re-execute this cell.')\n",
    "# else:\n",
    "#     print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "WfPiU9LaR9O7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of available CPUs is: 4\n"
     ]
    }
   ],
   "source": [
    "# Retrieve CPU information from the system\n",
    "ncpu = !nproc\n",
    "print(\"The number of available CPUs is:\", ncpu[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUw0fDwqR9O-"
   },
   "source": [
    "## 1.a) Load libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UpJD2SzRR9O-"
   },
   "outputs": [],
   "source": [
    "# Set the random seed number for reproducible results\n",
    "seedNum = 888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5yDoBs19R9PA"
   },
   "outputs": [],
   "source": [
    "# Load libraries and packages\n",
    "import random\n",
    "random.seed(seedNum)\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import string\n",
    "import nltk\n",
    "# import boto3\n",
    "# from dotenv import load_dotenv\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dTS_Cx_Y7nGi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/pythonml/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcIz9xXxR9PC"
   },
   "source": [
    "## 1.b) Set up the controlling parameters and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YecJPSrfR9PC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n",
      "TensorFlow version: 2.3.1\n"
     ]
    }
   ],
   "source": [
    "# Begin the timer for the script processing\n",
    "startTimeScript = datetime.now()\n",
    "\n",
    "# Set up the number of CPU cores available for multi-thread processing\n",
    "n_jobs = 1\n",
    "\n",
    "# Set up the flag to stop sending progress emails (setting to True will send status emails!)\n",
    "notifyStatus = False\n",
    "\n",
    "# Set the verbose level for program execution output\n",
    "verbose = False\n",
    "\n",
    "# Set Pandas options\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# Set the percentage sizes for splitting the dataset\n",
    "TEST_SET_SIZE = 0.2\n",
    "VAL_SET_SIZE = 0.25\n",
    "\n",
    "# Set the number of folds for cross validation\n",
    "N_FOLDS = 5\n",
    "N_ITERATIONS = 1\n",
    "\n",
    "# Set various default modeling parameters\n",
    "DEFAULT_LOSS = 'binary_crossentropy'\n",
    "DEFAULT_METRICS = ['accuracy']\n",
    "DEFAULT_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "DEFAULT_INITIALIZER = tf.keras.initializers.RandomNormal(seed=seedNum)\n",
    "CLASSIFIER_ACTIVATION = 'sigmoid'\n",
    "MAX_EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "MIN_OCCURANCE = 1\n",
    "NUM_CLASSES = 1\n",
    "DEFAULT_VECTOR_SPACE = 128\n",
    "DEFAULT_FILTERS = 32\n",
    "DEFAULT_KERNEL_SIZE = 16\n",
    "DEFAULT_POOL_SIZE = 2\n",
    "\n",
    "# Define the labels to use for graphing the data\n",
    "train_metric = \"accuracy\"\n",
    "validation_metric = \"val_accuracy\"\n",
    "train_loss = \"loss\"\n",
    "validation_loss = \"val_loss\"\n",
    "\n",
    "# Check the number of GPUs accessible through TensorFlow\n",
    "print('Num GPUs Available:', len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "# Print out the TensorFlow version for confirmation\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ggPdMUQfR9PE"
   },
   "outputs": [],
   "source": [
    "# Set up the parent directory location for loading the dotenv files\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# gdrivePrefix = '/content/gdrive/My Drive/Colab_Downloads/'\n",
    "# env_path = '/content/gdrive/My Drive/Colab Notebooks/'\n",
    "# dotenv_path = env_path + \"python_script.env\"\n",
    "# load_dotenv(dotenv_path=dotenv_path)\n",
    "\n",
    "# Set up the dotenv file for retrieving environment variables\n",
    "# env_path = \"/Users/david/PycharmProjects/\"\n",
    "# dotenv_path = env_path + \"python_script.env\"\n",
    "# load_dotenv(dotenv_path=dotenv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1j-y-SvmR9PG"
   },
   "outputs": [],
   "source": [
    "# Set up the email notification function\n",
    "def status_notify(msg_text):\n",
    "    access_key = os.environ.get('SNS_ACCESS_KEY')\n",
    "    secret_key = os.environ.get('SNS_SECRET_KEY')\n",
    "    aws_region = os.environ.get('SNS_AWS_REGION')\n",
    "    topic_arn = os.environ.get('SNS_TOPIC_ARN')\n",
    "    if (access_key is None) or (secret_key is None) or (aws_region is None):\n",
    "        sys.exit(\"Incomplete notification setup info. Script Processing Aborted!!!\")\n",
    "    sns = boto3.client('sns', aws_access_key_id=access_key, aws_secret_access_key=secret_key, region_name=aws_region)\n",
    "    response = sns.publish(TopicArn=topic_arn, Message=msg_text)\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] != 200 :\n",
    "        print('Status notification not OK with HTTP status code:', response['ResponseMetadata']['HTTPStatusCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KDY6p9pWR9PI"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 1 - Prepare Environment has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YQtFgiRNR9PJ"
   },
   "outputs": [],
   "source": [
    "# Reset the random number generators\n",
    "def reset_random(x):\n",
    "    random.seed(x)\n",
    "    np.random.seed(x)\n",
    "    tf.random.set_seed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "id": "7auUp3NTR9PL",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 1 - Prepare Environment completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkkVYtk_R9PN"
   },
   "source": [
    "# Task 2 - Load and Prepare Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "id": "pX4nz8fYR9PN",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 2 - Load and Prepare Text Data has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ReBalQ07nGk"
   },
   "source": [
    "## 2.a) Download Text Data Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "72uuLoG4R9PP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    40000 non-null  object\n",
      " 1   label   40000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 625.1+ KB\n",
      "None\n",
      "                                                text  label\n",
      "0  I grew up (b. 1965) watching and loving the Th...      0\n",
      "1  When I put this movie in my DVD player, and sa...      0\n",
      "2  Why do people who do not know what a particula...      0\n",
      "3  Even though I have great interest in Biblical ...      0\n",
      "4  Im a die hard Dads Army fan and nothing will e...      1\n",
      "5  A terrible movie as everyone has said. What ma...      0\n",
      "6  Finally watched this shocking movie last night...      1\n",
      "7  I caught this film on AZN on cable. It sounded...      0\n",
      "8  It may be the remake of 1987 Autumn's Tale aft...      1\n",
      "9  My Super Ex Girlfriend turned out to be a plea...      1\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'https://dainesanalytics.com/datasets/kaggle-imdb-movie-sentiment/Train.csv'\n",
    "Xy_train = pd.read_csv(dataset_path)\n",
    "\n",
    "# Take a peek at the dataframe after import\n",
    "print(Xy_train.info())\n",
    "print(Xy_train.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6rVQlQs8FTvH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   label   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 78.2+ KB\n",
      "None\n",
      "                                                text  label\n",
      "0  It's been about 14 years since Sharon Stone aw...      0\n",
      "1  someone needed to make a car payment... this i...      0\n",
      "2  The Guidelines state that a comment must conta...      0\n",
      "3  This movie is a muddled mish-mash of clichés f...      0\n",
      "4  Before Stan Laurel became the smaller half of ...      0\n",
      "5  This is the best movie I've ever seen! <br /><...      1\n",
      "6  The morbid Catholic writer Gerard Reve (Jeroen...      1\n",
      "7  \"Semana Santa\" or \"Angel Of Death\" is a very w...      0\n",
      "8  Somebody mastered the difficult task of mergin...      1\n",
      "9  Why did I waste 1.5 hours of my life watching ...      0\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'https://dainesanalytics.com/datasets/kaggle-imdb-movie-sentiment/Valid.csv'\n",
    "Xy_valid = pd.read_csv(dataset_path)\n",
    "\n",
    "# Take a peek at the dataframe after import\n",
    "print(Xy_valid.info())\n",
    "print(Xy_valid.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28VO7JDe7nGl"
   },
   "source": [
    "## 2.b) Splitting Data into Feature and Label Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LRnLzyfU7mVa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_df.shape: (40000,) y_train_df.shape: (40000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_df = Xy_train.iloc[:,0]\n",
    "y_train_df = Xy_train.iloc[:,1]\n",
    "print('X_train_df.shape: {} y_train_df.shape: {}'.format(X_train_df.shape, y_train_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YA8Seb7pFt1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_valid_df.shape: (5000,) y_valid_df.shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_valid_df = Xy_valid.iloc[:,0]\n",
    "y_valid_df = Xy_valid.iloc[:,1]\n",
    "print('X_valid_df.shape: {} y_valid_df.shape: {}'.format(X_valid_df.shape, y_valid_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gKw9ELQEHVxD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_complete_df.shape: (45000,) y_complete_df.shape: (45000,)\n"
     ]
    }
   ],
   "source": [
    "X_complete_df = pd.concat([X_train_df, X_valid_df], ignore_index=True)\n",
    "y_complete_df = pd.concat([y_train_df, y_valid_df], ignore_index=True)\n",
    "print('X_complete_df.shape: {} y_complete_df.shape: {}'.format(X_complete_df.shape, y_complete_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC6v3JH37nGl"
   },
   "source": [
    "## 2.c) Load Document and Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A6ajSXvMR9Pb"
   },
   "outputs": [],
   "source": [
    "# turn a sentence into clean tokens\n",
    "def clean_sentence(sentence):\n",
    "    # split into tokens by white space\n",
    "    tokens = sentence.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XXjOcu-e7nGl"
   },
   "outputs": [],
   "source": [
    "# load sentences and add to vocab\n",
    "def add_sentence_to_vocab(sentence, vocab):\n",
    "    # clean doc\n",
    "    tokens = clean_sentence(sentence)\n",
    "    # update counts\n",
    "    vocab.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P_6tzgMR7nGm"
   },
   "outputs": [],
   "source": [
    "# load all sentences in the dataframe\n",
    "def build_vocabulary(comments, vocab):\n",
    "    # walk through all comments in the dataframe\n",
    "    for i in range(len(comments)):\n",
    "        # add comments to vocab\n",
    "        sentence = comments.iloc[i]\n",
    "        add_sentence_to_vocab(sentence, vocab)\n",
    "        if verbose : print('Processing comment:', sentence)\n",
    "        if ((i+1) % 100000) == 0 : print(i+1, 'comments processed so far.')\n",
    "    print('Total number of comments loaded into the vocabulary:', i+1, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yux8fNWF7nGm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of comments loaded into the vocabulary: 45000 \n",
      "\n",
      "The total number of words in the vocabulary: 195713\n",
      "The top 50 words in the vocabulary:\n",
      " [('br', 102476), ('The', 79702), ('movie', 74201), ('film', 65917), ('one', 41670), ('like', 33799), ('This', 26243), ('good', 24690), ('It', 21382), ('would', 21355), ('time', 20444), ('really', 20025), ('see', 19639), ('even', 19348), ('story', 19257), ('much', 16539), ('get', 16019), ('people', 15037), ('bad', 14970), ('great', 14847), ('well', 14162), ('first', 13793), ('also', 13724), ('make', 13676), ('made', 13654), ('movies', 13619), ('films', 13560), ('could', 13413), ('way', 13161), ('dont', 13047), ('characters', 12999), ('But', 12592), ('think', 12516), ('And', 11900), ('Its', 11823), ('seen', 11695), ('character', 11505), ('watch', 11271), ('many', 11235), ('never', 10931), ('acting', 10908), ('know', 10907), ('plot', 10833), ('two', 10802), ('In', 10515), ('little', 10469), ('love', 10403), ('show', 10341), ('best', 10190), ('ever', 10070)]\n"
     ]
    }
   ],
   "source": [
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "build_vocabulary(X_complete_df, vocab)\n",
    "# print the size of the vocab\n",
    "print('The total number of words in the vocabulary:', len(vocab))\n",
    "# print the top words in the vocab\n",
    "top_words = 50\n",
    "print('The top', top_words, 'words in the vocabulary:\\n', vocab.most_common(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Ud1BHLtN7nGm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words with the minimum appearance: 195713\n"
     ]
    }
   ],
   "source": [
    "# keep tokens with a min occurrence in the vocabulary\n",
    "tokens = [k for k,c in vocab.items() if c >= MIN_OCCURANCE]\n",
    "print('The number of words with the minimum appearance:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DjOk2QOE7nGm"
   },
   "outputs": [],
   "source": [
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocabulary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iIk6Zyc7nGm"
   },
   "source": [
    "## 2.d) Create Tokenizer and Encode the Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "m-9yNpBk7nGm"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "P7y0pyxw7nGn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the vocabulary: 195713\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocabulary.txt'\n",
    "vocab_text = load_doc(vocab_filename)\n",
    "vocab_words = vocab_text.split()\n",
    "vocab = set(vocab_words)\n",
    "print('Number of tokens in the vocabulary:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xSFRP96g7nGn"
   },
   "outputs": [],
   "source": [
    "# load comments, clean and return line of tokens\n",
    "def comment_to_line(sentence, vocab):\n",
    "    # clean sentence\n",
    "    tokens = clean_sentence(sentence)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "MCAkwDu67nGn"
   },
   "outputs": [],
   "source": [
    "# load all comments in a directory\n",
    "def process_comments_to_lines(comments, vocab):\n",
    "    lines = list()\n",
    "    # walk through all comments in the dataframe\n",
    "    for i in range(len(comments)):\n",
    "        # load and clean the comments\n",
    "        sentence = comments.iloc[i]\n",
    "        line = comment_to_line(sentence, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "9AUBKiKC7nGn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments processed: 40000\n"
     ]
    }
   ],
   "source": [
    "# Load all training cases\n",
    "train_docs = process_comments_to_lines(X_train_df, vocab)\n",
    "print('Number of comments processed:', len(train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments processed: 5000\n"
     ]
    }
   ],
   "source": [
    "# Load all validation cases\n",
    "valid_docs = process_comments_to_lines(X_valid_df, vocab)\n",
    "print('Number of comments processed:', len(valid_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "fW6CsVLq7nGn"
   },
   "outputs": [],
   "source": [
    "# prepare word-embedding encoding of training dataset\n",
    "def encode_train_data(docs_set, maxlen):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(docs_set)\n",
    "    # encode training data set\n",
    "    encoded_docs = tokenizer.texts_to_sequences(docs_set)\n",
    "    # pad sequences\n",
    "    train_encoded = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "    # define vocabulary size (largest integer value)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return train_encoded, tokenizer, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QMi2c08h7nGn"
   },
   "outputs": [],
   "source": [
    "# prepare word-embedding encoding of test dataset\n",
    "def encode_valid_data(docs_set, tokenizer, maxlen):\n",
    "    # encode validation data set\n",
    "    encoded_docs = tokenizer.texts_to_sequences(docs_set)\n",
    "    # pad sequences\n",
    "    val_encoded = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "    return val_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "5tm0eZHU7nGn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum document length: 1480\n"
     ]
    }
   ],
   "source": [
    "# Get maximum doc length for padding sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('The maximum document length:', max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2CVIIZTG7nGo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of encoded vocabulary: 154729\n",
      "The shape of the encoded training dataset: (40000, 1480)\n"
     ]
    }
   ],
   "source": [
    "# Encode the training dataset\n",
    "X_train, trained_tokenizer, vocabulary_size = encode_train_data(train_docs, max_length)\n",
    "print('The size of encoded vocabulary:', vocabulary_size)\n",
    "print('The shape of the encoded training dataset:', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "NhZyVIvK7nGo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded test classes: (40000,)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train_df.values.ravel()\n",
    "print('The shape of the encoded test classes:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded validation dataset: (5000, 1480)\n"
     ]
    }
   ],
   "source": [
    "# Encode the validation dataset\n",
    "X_valid = encode_valid_data(valid_docs, trained_tokenizer, max_length)\n",
    "print('The shape of the encoded validation dataset:', X_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training classes: (40000,)\n"
     ]
    }
   ],
   "source": [
    "y_valid = y_valid_df.values.ravel()\n",
    "print('The shape of the encoded training classes:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "JdeBWbYeR9Pi"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 2 - Load and Prepare Text Data completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGVOq6P6R9Pj"
   },
   "source": [
    "# Task 3 - Define and Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "W52KEpCWR9Pk"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 3 - Define and Train Models has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "B53Gp_as7nGo"
   },
   "outputs": [],
   "source": [
    "# Define the baseline model for benchmarking\n",
    "def create_nn_model(input_param=max_length, output_param=NUM_CLASSES, n_vocab=vocabulary_size, n_vectors=DEFAULT_VECTOR_SPACE, n_pools=DEFAULT_POOL_SIZE, \n",
    "                    conv1_filters=DEFAULT_FILTERS, conv1_kernels=DEFAULT_KERNEL_SIZE, dense_nodes=64, opt_param=DEFAULT_OPTIMIZER):\n",
    "    nn_model = keras.Sequential([\n",
    "        layers.Embedding(n_vocab, n_vectors, input_length=input_param),\n",
    "        layers.Conv1D(filters=conv1_filters, kernel_size=conv1_kernels, activation='relu', kernel_initializer=DEFAULT_INITIALIZER),\n",
    "        layers.MaxPooling1D(pool_size=n_pools),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(dense_nodes, activation='relu', kernel_initializer=DEFAULT_INITIALIZER),\n",
    "        layers.Dense(output_param, activation=CLASSIFIER_ACTIVATION)\n",
    "    ])\n",
    "    nn_model.compile(loss=DEFAULT_LOSS, optimizer=opt_param, metrics=DEFAULT_METRICS)\n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zDjkpVZu7nGo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 1014s 2s/step - loss: 0.3418 - accuracy: 0.8345 - val_loss: 0.2495 - val_accuracy: 0.8946\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1005s 2s/step - loss: 0.0683 - accuracy: 0.9771 - val_loss: 0.3317 - val_accuracy: 0.8844\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1005s 2s/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.4859 - val_accuracy: 0.8834\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 994s 2s/step - loss: 1.3641e-04 - accuracy: 1.0000 - val_loss: 0.5385 - val_accuracy: 0.8842\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 996s 2s/step - loss: 4.6467e-05 - accuracy: 1.0000 - val_loss: 0.5997 - val_accuracy: 0.8836\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 995s 2s/step - loss: 2.6681e-05 - accuracy: 1.0000 - val_loss: 0.6302 - val_accuracy: 0.8852\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 991s 2s/step - loss: 1.5004e-05 - accuracy: 1.0000 - val_loss: 0.6424 - val_accuracy: 0.8838\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 997s 2s/step - loss: 1.1015e-05 - accuracy: 1.0000 - val_loss: 0.6750 - val_accuracy: 0.8838\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1006s 2s/step - loss: 4.4021e-06 - accuracy: 1.0000 - val_loss: 0.7357 - val_accuracy: 0.8848\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1018s 2s/step - loss: 2.3470e-06 - accuracy: 1.0000 - val_loss: 0.7676 - val_accuracy: 0.8840\n",
      "Total time for model fitting: 2:47:17.212003\n"
     ]
    }
   ],
   "source": [
    "# Initialize the default model and get a baseline result\n",
    "startTimeModule = datetime.now()\n",
    "reset_random(seedNum)\n",
    "baseline_model = create_nn_model()\n",
    "baseline_model_history = baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCH,\n",
    "\tvalidation_data=(X_valid, y_valid),\n",
    "    verbose=1)\n",
    "print('Total time for model fitting:', (datetime.now() - startTimeModule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "id": "5vzQkIfyR9Ps",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 3 - Define and Train Models completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmuo6X06R9Pu"
   },
   "source": [
    "# Task 4 - Evaluate and Optimize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "id": "h-Xp746uR9Pu",
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 4 - Evaluate and Optimize Models has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "7kpjFGr_7nGp"
   },
   "outputs": [],
   "source": [
    "# Not applicable for this iteration of modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "6n7j3PwoR9P0"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 4 - Evaluate and Optimize Models completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRcXHaGhR9P1"
   },
   "source": [
    "# Task 5 - Finalize Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "p3tj_9tBR9P2"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 5 - Finalize Model and Make Predictions has begun on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.a) Train the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments processed: 45000\n"
     ]
    }
   ],
   "source": [
    "# Load all training cases\n",
    "complete_docs = process_comments_to_lines(X_complete_df, vocab)\n",
    "print('Number of comments processed:', len(complete_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum document length: 1480\n"
     ]
    }
   ],
   "source": [
    "# Get maximum doc length for padding sequences\n",
    "final_max_length = max([len(s.split()) for s in train_docs])\n",
    "print('The maximum document length:', final_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of encoded vocabulary: 165150\n",
      "The shape of the encoded final dataset: (45000, 1480)\n"
     ]
    }
   ],
   "source": [
    "# Encode the training dataset\n",
    "X_complete, final_tokenizer, final_vocabulary_size = encode_train_data(complete_docs, final_max_length)\n",
    "print('The size of encoded vocabulary:', final_vocabulary_size)\n",
    "print('The shape of the encoded final dataset:', X_complete.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded training classes: (45000,)\n"
     ]
    }
   ],
   "source": [
    "y_complete = y_complete_df.values.ravel()\n",
    "print('The shape of the encoded training classes:', y_complete.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "B53Gp_as7nGo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "704/704 [==============================] - 1130s 2s/step - loss: 0.3409 - accuracy: 0.8384\n",
      "Epoch 2/10\n",
      "704/704 [==============================] - 1136s 2s/step - loss: 0.0695 - accuracy: 0.9766\n",
      "Epoch 3/10\n",
      "704/704 [==============================] - 1147s 2s/step - loss: 0.0041 - accuracy: 0.9991\n",
      "Epoch 4/10\n",
      "704/704 [==============================] - 1123s 2s/step - loss: 2.1416e-04 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "704/704 [==============================] - 1131s 2s/step - loss: 4.8620e-05 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "704/704 [==============================] - 1139s 2s/step - loss: 2.4847e-05 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "704/704 [==============================] - 1145s 2s/step - loss: 1.4215e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "704/704 [==============================] - 1157s 2s/step - loss: 8.4861e-06 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "704/704 [==============================] - 1141s 2s/step - loss: 5.2091e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "704/704 [==============================] - 1134s 2s/step - loss: 3.2378e-06 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1c79db9790>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the final model for evaluating the test dataset\n",
    "FINAL_OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "reset_random(seedNum)\n",
    "final_model = create_nn_model(input_param=final_max_length, n_vocab=final_vocabulary_size, opt_param=FINAL_OPTIMIZER)\n",
    "final_model.fit(X_complete, y_complete, epochs=MAX_EPOCH, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "xySH-3e77nGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1480, 128)         21139200  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1465, 32)          65568     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 732, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 23424)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                1499200   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 22,704,033\n",
      "Trainable params: 22,704,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize the final model\n",
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.b) Load Test Dataset and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Px0NEI3EEU4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   label   5000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 78.2+ KB\n",
      "None\n",
      "                                                text  label\n",
      "0  I always wrote this series off as being a comp...      0\n",
      "1  1st watched 12/7/2002 - 3 out of 10(Dir-Steve ...      0\n",
      "2  This movie was so poorly written and directed ...      0\n",
      "3  The most interesting thing about Miryang (Secr...      1\n",
      "4  when i first read about \"berlin am meer\" i did...      0\n",
      "5  I saw this film on September 1st, 2005 in Indi...      1\n",
      "6  I saw a screening of this movie last night. I ...      0\n",
      "7  William Hurt may not be an American matinee id...      1\n",
      "8  IT IS A PIECE OF CRAP! not funny at all. durin...      0\n",
      "9  I'M BOUT IT(1997)<br /><br />Developed & publi...      0\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'https://dainesanalytics.com/datasets/kaggle-imdb-movie-sentiment/Test.csv'\n",
    "Xy_test = pd.read_csv(dataset_path)\n",
    "\n",
    "# Take a peek at the dataframe after import\n",
    "print(Xy_test.info())\n",
    "print(Xy_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "KvEWAjfmGDrt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_df.shape: (5000,) y_test_df.shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "X_test_df = Xy_test.iloc[:,0]\n",
    "y_test_df = Xy_test.iloc[:,1]\n",
    "print(\"X_test_df.shape: {} y_test_df.shape: {}\".format(X_test_df.shape, y_test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments processed: 5000\n"
     ]
    }
   ],
   "source": [
    "# Load all test cases\n",
    "test_docs = process_comments_to_lines(X_test_df, vocab)\n",
    "print('Number of comments processed:', len(test_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare word-embedding encoding of test dataset\n",
    "def encode_test_data(docs_set, tokenizer, maxlen):\n",
    "    # encode validation data set\n",
    "    encoded_docs = tokenizer.texts_to_sequences(docs_set)\n",
    "    # pad sequences\n",
    "    val_encoded = pad_sequences(encoded_docs, maxlen=maxlen, padding='post')\n",
    "    return val_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "9_ien2g07nGp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded test dataset: (5000, 1480)\n"
     ]
    }
   ],
   "source": [
    "# encode test dataset\n",
    "X_test = encode_test_data(test_docs, final_tokenizer, final_max_length)\n",
    "print('The shape of the encoded test dataset:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the encoded testing classes: (5000,)\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test_df.values.ravel()\n",
    "print('The shape of the encoded testing classes:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "ieIRL6rr7nGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8966\n",
      "[[2230  265]\n",
      " [ 252 2253]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90      2495\n",
      "           1       0.89      0.90      0.90      2505\n",
      "\n",
      "    accuracy                           0.90      5000\n",
      "   macro avg       0.90      0.90      0.90      5000\n",
      "weighted avg       0.90      0.90      0.90      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test_predictions = final_model.predict(X_test, batch_size=default_batch, verbose=1)\n",
    "test_predictions = (final_model.predict(X_test) > 0.5).astype(\"int32\").ravel()\n",
    "print('Accuracy Score:', accuracy_score(y_test, test_predictions))\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "S47yeSrOR9P-"
   },
   "outputs": [],
   "source": [
    "if notifyStatus: status_notify('(TensorFlow Text Classification) Task 5 - Finalize Model and Make Predictions completed on ' + datetime.now().strftime('%A %B %d, %Y %I:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "n_kTqSp4R9QA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for the script: 6:02:33.891198\n"
     ]
    }
   ],
   "source": [
    "print ('Total time for the script:',(datetime.now() - startTimeScript))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "py_nlp_binaryclass_imdb_movie_sentiment_take2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
